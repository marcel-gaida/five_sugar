{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Script for Reddit - Credentials in passcode file - using PRAWC - change password_file_path to your systems"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-08T23:23:10.140251Z",
     "end_time": "2023-04-08T23:23:10.157151Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import praw\n",
    "\n",
    "# Set the path to the password file\n",
    "password_file_path = r\"C:\\Users\\USERNAME\\OneDrive\\Documents\\GitHub\\non_share\\passcode.txt\"\n",
    "\n",
    "# Open the password file and read its contents\n",
    "with open(password_file_path, 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Split the contents into lines\n",
    "lines = contents.split('\\n')\n",
    "\n",
    "# Initialize variables to store the client ID and secret key\n",
    "CLIENT_ID = None\n",
    "SECRET_KEY = None\n",
    "pw_reddit = None\n",
    "\n",
    "# Loop through the lines to find the client ID and secret key\n",
    "for line in lines:\n",
    "    if line.startswith('CLIENT_ID'):\n",
    "        CLIENT_ID = line.split('=')[1].strip()\n",
    "    elif line.startswith('SECRET_KEY'):\n",
    "        SECRET_KEY = line.split('=')[1].strip()\n",
    "    elif line.startswith('pw_reddit'):\n",
    "        pw_reddit = line.split('=')[1].strip()\n",
    "    elif line.startswith('reddit_user'):\n",
    "        reddit_user = line.split('=')[1].strip()\n",
    "\n",
    "# Print the client ID and secret key\n",
    "print(f'CLIENT_ID: {CLIENT_ID}')\n",
    "print(f'SECRET_KEY: {SECRET_KEY}')\n",
    "print(f'pw_reddit: {pw_reddit}')\n",
    "print(f'reddit_user: {reddit_user}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define User_Agent - Follow Reddit PRAW Guidelines for API Access"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id = CLIENT_ID,\n",
    "    client_secret = SECRET_KEY,\n",
    "    #redirect_uri = 'https://localhost:8080',\n",
    "    username = reddit_user,\n",
    "    user_agent = 'ScriptCounterAPI/0.0.1 by ****',\n",
    ")\n",
    "\n",
    "# Authenticate the Reddit instance by accessing your user account\n",
    "try:\n",
    "    reddit.user.me()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Print the access token for your account\n",
    "print(reddit.auth.scopes())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T23:23:11.412738Z",
     "end_time": "2023-04-08T23:23:11.673702Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test API - Access - change subreddit name your needs. top limit will retrieve the first 10 Top submissions in the subreddit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for submission in reddit.subreddit('dataisbeautiful').top(limit=10):\n",
    "    print(submission.title)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T23:03:51.664527Z",
     "end_time": "2023-04-08T23:03:52.600081Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If needed change access to read_only mode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#reddit.read_only = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T12:42:29.326241Z",
     "end_time": "2023-04-04T12:42:29.333756Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('dataisbeautiful')\n",
    "hot_dataib = subreddit.hot(limit=10)\n",
    "for submission in hot_dataib:\n",
    "    print(submission.title)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T15:28:51.608643Z",
     "end_time": "2023-04-04T15:28:51.991721Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import submissions in to Panda dataframe and use exclusion to remove"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get subreddit\n",
    "subreddit = reddit.subreddit('dataisbeautiful')\n",
    "\n",
    "# List of excluded words\n",
    "exclusions = ['is', 'not', 'and', 'or', 'oc', '[oc]','how','many','you','need','to','of','if','by','the','i','for','a','-','an','with','on','—','_','as']\n",
    "\n",
    "# Dictionary to store word counts\n",
    "word_counts = {}\n",
    "\n",
    "# Iterate through all posts in subreddit\n",
    "for post in subreddit.new(limit=1000):\n",
    "    # Split post title into words and convert to lowercase\n",
    "    words = post.title.lower().split()\n",
    "    # Iterate through words and update word counts\n",
    "    for word in words:\n",
    "        if word not in exclusions:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "\n",
    "# Create DataFrame from word_counts dictionary\n",
    "df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n",
    "\n",
    "# Write DataFrame to Excel file\n",
    "df.to_excel('word_counts.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T15:42:40.362479Z",
     "end_time": "2023-04-04T15:44:24.157611Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subreddit_name = 'dataisbeautiful'\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "top_submissions = subreddit.top(limit=10)  # get the top 10 submissions in the subreddit\n",
    "\n",
    "for submission in top_submissions:\n",
    "    print(submission.title)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T22:37:29.018189Z",
     "end_time": "2023-04-08T22:37:29.863088Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using Exclusion List on retrieved submissions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get subreddit\n",
    "subreddit = reddit.subreddit('dataisbeautiful')\n",
    "\n",
    "# List of excluded words\n",
    "exclusions = ['is', 'not', 'and', 'or', 'oc', '[oc]','how','many','you','need','to','of','if','by','the','i','for','a','-','an','with','on','—','_','as','in','from','are','at','what','visualized','visualization','&']\n",
    "\n",
    "# Dictionary to store word counts\n",
    "word_counts = {}\n",
    "\n",
    "# Counter variable\n",
    "count = 0\n",
    "\n",
    "# Loop through subreddit\n",
    "for post in subreddit.new(limit=None):\n",
    "    # Split post title into words and convert to lowercase\n",
    "    words = post.title.lower().split()\n",
    "    # Iterate through words and update word counts\n",
    "    for word in words:\n",
    "        if word not in exclusions:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "    # Increment counter\n",
    "    count += 1\n",
    "    # Break loop after two rounds\n",
    "    if count >= 3000:\n",
    "        break\n",
    "\n",
    "# Create DataFrame from word_counts dictionary\n",
    "df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n",
    "\n",
    "# Write DataFrame to Excel file\n",
    "df.to_excel('word_counts.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T22:20:13.932798Z",
     "end_time": "2023-04-04T22:21:58.199117Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exporting to Excel in Batches - using regex to clean the title from special characters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import re\n",
    "\n",
    "subreddit_name = 'dataisbeautiful'\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "post_titles = []\n",
    "post_dates = []\n",
    "for post in subreddit.new(limit=3000):\n",
    "    title = post.title.lower()  # convert to lowercase\n",
    "    title = re.sub(r'[^a-zA-Z0-9\\s]', '', title)  # remove special characters\n",
    "    post_titles.append(title)\n",
    "    post_dates.append(post.created_utc)\n",
    "\n",
    "df = pd.DataFrame({'text': post_titles, 'Date': post_dates})\n",
    "df['Date'] = pd.to_datetime(df['Date'], unit='s')  # Convert UNIX timestamps to datetime objects\n",
    "\n",
    "file_exists = False\n",
    "try:\n",
    "    with pd.ExcelFile('reddit_posts.xlsx') as xls:\n",
    "        file_exists = True\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with pd.ExcelWriter('reddit_posts.xlsx', engine='openpyxl', mode='a' if file_exists else 'w') as writer:\n",
    "    sheet_name = 'dataisbeautiful'\n",
    "    df.to_excel(writer, index=False, header=not file_exists, sheet_name=sheet_name, if_sheet_exists=\"replace\")\n",
    "\n",
    "print(f\"Exported {len(df)} posts to reddit_posts.xlsx\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T23:12:38.214139Z",
     "end_time": "2023-04-08T23:14:12.523451Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-09T18:10:22.295749Z",
     "end_time": "2023-04-09T18:10:22.349838Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using Pushshift API to retrieve Reddit posts in batches for i in range(0, 5000 endpoint, 100 submissions per batch). Even if the final export to excel should fail - the submissions should be in the pandas dataframe - you can check the dataframe with the code below. If the export to Excel should fail try to export it to CSV."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI\n",
    "import signal\n",
    "\n",
    "# Set the subreddit name and endpoint URL\n",
    "subreddit_name = 'dataisbeautiful'\n",
    "\n",
    "# Initialize Pushshift API\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# Create an empty dataframe to store the posts\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Define a signal handler to gracefully exit the script\n",
    "def signal_handler(signal, frame):\n",
    "    print(\"\\nKeyboardInterrupt detected. Exiting script.\")\n",
    "    df.to_excel('reddit_posts.xlsx', index=False)\n",
    "    exit(0)\n",
    "\n",
    "# Set the signal handler for SIGINT\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "# Loop through the subreddit in batches of 100 with a delay of 1 second in between\n",
    "post_list = []\n",
    "max_attempts = 3\n",
    "attempt_count = 0\n",
    "for i in range(0, 5000, 100):\n",
    "    # Construct the API request\n",
    "    request = {\n",
    "        \"subreddit\": subreddit_name,\n",
    "        \"size\": 100,\n",
    "        \"after\": f\"{i}d\"\n",
    "    }\n",
    "\n",
    "    # Send the API request\n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            data = api.search_submissions(**request)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Waiting for 60 seconds before retrying...\")\n",
    "            time.sleep(60)\n",
    "            attempts += 1\n",
    "    else:\n",
    "        print(\"Reached maximum number of attempts. Skipping batch.\")\n",
    "        continue\n",
    "\n",
    "    # Append the posts to the list\n",
    "    for post in data:\n",
    "        post_list.append({\n",
    "            'id': post['id'],\n",
    "            'title': post['title'],\n",
    "            'author': post['author'] if 'author' in post else None,\n",
    "            'created_utc': post['created_utc'],\n",
    "            'score': post['score'],\n",
    "            'num_comments': post['num_comments'],\n",
    "            'permalink': f\"https://www.reddit.com{post['permalink']}\",\n",
    "            'url': post['url']\n",
    "        })\n",
    "\n",
    "    # Print status message\n",
    "    print(f\"Batch added - current item number: {i+100}\")\n",
    "\n",
    "    # Concatenate the list to the dataframe every 100 posts\n",
    "    if len(post_list) >= 100:\n",
    "        df = pd.concat([df, pd.DataFrame(post_list)], ignore_index=True)\n",
    "        post_list = []\n",
    "        time.sleep(1)\n",
    "\n",
    "# Concatenate remaining posts to dataframe\n",
    "if post_list:\n",
    "    df = pd.concat([df, pd.DataFrame(post_list)], ignore_index=True)\n",
    "\n",
    "# Export DataFrame to Excel file\n",
    "df.to_excel('reddit_posts.xlsx', index=False)\n",
    "\n",
    "print(f\"Exported {len(df)} posts to reddit_posts.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-10T10:44:39.773156Z",
     "end_time": "2023-04-10T10:51:46.047188Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display pandas dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-10T15:42:56.876544Z",
     "end_time": "2023-04-10T15:42:56.953130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export DataFrame to Excel file\n",
    "df.to_excel('batch_reddit_posts.xlsx', index=False)\n",
    "\n",
    "print(f\"Exported {len(df)} posts to 5000 batch_reddit_posts.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the DataFrame to a CSV file\n",
    "df.to_csv('my_data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-10T15:46:40.515710Z",
     "end_time": "2023-04-10T15:46:43.284953Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
